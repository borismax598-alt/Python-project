{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0flaw7HdTLC"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The first step in our task is to obtain the data necessary for analysis. Since our company is in the early stages of development and does not have its own database, we intend to use publicly available resources.  \n",
    "  \n",
    "For this purpose, we have been recommended the website [Scrape This Site](https://www.scrapethissite.com/pages/forms/). However, before we start downloading data, it is important to carefully review the [FAQ](https://www.scrapethissite.com/faq/) section on the site. Particular attention should be paid to the restrictions on the number of requests, which is crucial for our solution.  \n",
    "  \n",
    "It is expected that after executing the code contained in this notebook, the `data/raw/` folder will be populated with data, which will serve as the source for the next stage of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U3qrIFCdTLF"
   },
   "source": [
    "# Notebook Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8i_DaiDdTLF"
   },
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\boris\\anaconda3\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\boris\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\boris\\anaconda3\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9ppX5CyndTLG"
   },
   "outputs": [],
   "source": [
    "import selenium as sel\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7zC6meddTLG"
   },
   "source": [
    "## Driver and Selenium Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.scrapethissite.com/pages/forms/\")\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgv7K-lQdTLG"
   },
   "source": [
    "# Fetching Website Content\n",
    "\n",
    "This section of the notebook contains code for fetching website content. To properly execute the task, consider the following steps:  \n",
    "- Ensure all available data on the site has been fetched by checking if there are additional data pages.  \n",
    "- Locate the data of interest on the page using `html` inspection tools.  \n",
    "- Navigate between subsequent data pages using browser mechanisms or by analyzing the `url` structure.  \n",
    "  \n",
    "> Remember to respect the query limits specified in the `FAQ`!  \n",
    "  \n",
    "Save the fetched data to the folder `data/raw/hockey_teams_page_{page_number}.html`. At this stage, we are retrieving data without processing it - analysis will be performed later.  \n",
    "  \n",
    "To fetch the `html` content of the page, you can use `browser.page_source`. Make sure the browser tool configuration (e.g., Selenium) is ready for use.  \n",
    "  \n",
    "> (Optional) If there are multiple pages to fetch, use the [zfill](https://www.programiz.com/python-programming/methods/string/zfill) function to maintain order in file names by adding leading zeros to the page numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lCOc6J3ldTLH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_01.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_02.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_03.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_04.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_05.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_06.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_07.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_08.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_09.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_10.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_11.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_12.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_13.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_14.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_15.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_16.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_17.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_18.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_19.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_20.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_21.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_22.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_23.html\n",
      "Saved: c:\\Users\\boris\\OneDrive\\Plocha\\Hockey copy\\Hockey_project\\data\\raw\\hockey_teams_page_24.html\n",
      "Done downloading data.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "raw = Path.cwd() / 'data' / 'raw'\n",
    "raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for page in range(1, 25):\n",
    "    url = f\"https://www.scrapethissite.com/pages/forms/?page_num={page}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    \n",
    "    filename = raw / f'hockey_teams_page_{page:02d}.html'\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "driver.quit()\n",
    "print(\"Done downloading data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ycm9QC8dTLH"
   },
   "source": [
    "# Summary\n",
    "\n",
    "Downloading raw data from our source has reduced the risk of problems stemming from site updates during the extraction process. This method also offers an additional benefit: it allows easy access to the data in its original form, which is crucial if reprocessing is needed.\n",
    "\n",
    "In the next step, we will focus on extracting the necessary information from the `html` pages, which is essential for conducting the analysis."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
