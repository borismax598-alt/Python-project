{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "558XhVVXfGK8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The purpose of this task is to extract and save information about hockey teams into JSON format, based on data from files located in the `/data/raw` directory, which were generated in the previous stage. The information to be scraped and saved includes:  \n",
        "\n",
        "- Team Name (`Team Name`),  \n",
        "- Year (`Year`),  \n",
        "- Number of wins (`Wins`),  \n",
        "- Number of losses (`Losses`),  \n",
        "- Number of overtime losses (`OT Losses` - Overtime Losses),  \n",
        "- Win percentage (`Win %`),  \n",
        "- Number of goals scored (`Goals For (GF)`),  \n",
        "- Number of goals conceded (`Goals Against (GA)`),  \n",
        "- Goal differential (`+ / -`).  \n",
        "\n",
        "Each collected record should be organized into a dictionary with the structure shown below and then added to the results list:  \n",
        "\n",
        "```python  \n",
        "{  \n",
        "    'Team Name': 'Boston Bruins',  \n",
        "    'Year': '1990',  \n",
        "    'Wins': '44',  \n",
        "    'Losses': '24',  \n",
        "    'OT Losses': '',  \n",
        "    'Win %': '0.55',  \n",
        "    'Goals For (GF)': '299',  \n",
        "    'Goals Against (GA)': '264',  \n",
        "    '+ / -': '35'  \n",
        "}  \n",
        "```\n",
        "\n",
        "Place each item into the results list.\n",
        "\n",
        "The resulting data should be saved in a file named hockey_teams.json, which will be placed in the `data/interim/` folder. This file will serve as a data source for further analysis in the next part of the workshop.\n",
        "\n",
        "> At this point, converting HTML to JSON may seem complex and unnecessary, but it aims to consolidate knowledge regarding this data structure due to its universality and prevalence not only in the world of data analysis but generally in IT as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u5hG5rOfGK_"
      },
      "source": [
        "# Notebook Configuration\n",
        "\n",
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lVWklch6fGLA"
      },
      "outputs": [],
      "source": [
        "import selenium as sel\n",
        "from selenium import webdriver\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E37pRB3JfGLA"
      },
      "source": [
        "# Scraping\n",
        "\n",
        "To scrape the required information from the saved files, follow these steps:\n",
        "\n",
        "1. Find all HTML files in the `data/raw` folder using the `glob` module.\n",
        "2. For each HTML file, use `BeautifulSoup` to scrape the page and extract the needed data.\n",
        "3. Save the obtained data as partially processed in the `hockey_teams.json` file located in the `/data/interim/` folder.\n",
        "\n",
        "These steps will allow for efficient processing of data from HTML files and prepare them for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxAHWKh7fGLB"
      },
      "source": [
        "## List of HTML files\n",
        "\n",
        "Using the `glob` module, find all `html` files in the `data/raw` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kd6AfGWsfGLB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data\\raw\\hockey_teams_page_01.html\n",
            "data\\raw\\hockey_teams_page_02.html\n",
            "data\\raw\\hockey_teams_page_03.html\n",
            "data\\raw\\hockey_teams_page_04.html\n",
            "data\\raw\\hockey_teams_page_05.html\n",
            "data\\raw\\hockey_teams_page_06.html\n",
            "data\\raw\\hockey_teams_page_07.html\n",
            "data\\raw\\hockey_teams_page_08.html\n",
            "data\\raw\\hockey_teams_page_09.html\n",
            "data\\raw\\hockey_teams_page_10.html\n",
            "data\\raw\\hockey_teams_page_11.html\n",
            "data\\raw\\hockey_teams_page_12.html\n",
            "data\\raw\\hockey_teams_page_13.html\n",
            "data\\raw\\hockey_teams_page_14.html\n",
            "data\\raw\\hockey_teams_page_15.html\n",
            "data\\raw\\hockey_teams_page_16.html\n",
            "data\\raw\\hockey_teams_page_17.html\n",
            "data\\raw\\hockey_teams_page_18.html\n",
            "data\\raw\\hockey_teams_page_19.html\n",
            "data\\raw\\hockey_teams_page_20.html\n",
            "data\\raw\\hockey_teams_page_21.html\n",
            "data\\raw\\hockey_teams_page_22.html\n",
            "data\\raw\\hockey_teams_page_23.html\n",
            "data\\raw\\hockey_teams_page_24.html\n"
          ]
        }
      ],
      "source": [
        "path_file = Path('data/raw')\n",
        "files = glob(str(path_file / '*.html'))\n",
        "for file in files:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigRm-unfGLB"
      },
      "source": [
        "## Scraping\n",
        "\n",
        "Extract data from `html` files, making sure to maintain the expected structure of a single record:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'Team Name': 'Boston Bruins',\n",
        "    'Year': '1990',\n",
        "    'Wins': '44',\n",
        "    'Losses': '24',\n",
        "    'OT Losses': '',\n",
        "    'Win %': '0.55',\n",
        "    'Goals For (GF)': '299',\n",
        "    'Goals Against (GA)': '264',\n",
        "    '+ / -': '35'\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4Ls8zhgBfKv_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of teams: 582\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "all_teams = []\n",
        "\n",
        "html_files = glob('data/raw/hockey_teams_page_*.html')\n",
        "\n",
        "for file in html_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        html = f.read()\n",
        "\n",
        "    soup = bs(html, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "\n",
        "    for row in rows[1:]:\n",
        "        cells = row.find_all('td')\n",
        "\n",
        "        team_data = {\n",
        "            'Team Name':          cells[0].text.strip(),\n",
        "            'Year':               cells[1].text.strip(),\n",
        "            'Wins':               cells[2].text.strip(),\n",
        "            'Losses':             cells[3].text.strip(),\n",
        "            'OT Losses':          cells[4].text.strip(),\n",
        "            'Win %':              cells[5].text.strip(),\n",
        "            'Goals For (GF)':     cells[6].text.strip(),\n",
        "            'Goals Against (GA)': cells[7].text.strip(),\n",
        "            '+ / -':              cells[8].text.strip(),\n",
        "        }\n",
        "\n",
        "        all_teams.append(team_data)\n",
        "\n",
        "print(f\"Total number of teams: {len(all_teams)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96tkK70QfGLB"
      },
      "source": [
        "# Summary\n",
        "\n",
        "After extracting the relevant information, the final step in preparation for analysis is to save the data to disk.\n",
        "\n",
        "### Saving the file\n",
        "Here, save the data to `data/interim/` and name the file `hockey_teams.json`\n",
        "\n",
        "> Note: Remember to import the appropriate library for handling the JSON format beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VY4qaMYpfGLC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of teams: 582\n",
            "First team: {'Team Name': 'Boston Bruins', 'Year': '1990', 'Wins': '44', 'Losses': '24', 'OT Losses': '', 'Win %': '0.55', 'Goals For (GF)': '299', 'Goals Against (GA)': '264', '+ / -': '35'}\n",
            "Last team: {'Team Name': 'Winnipeg Jets', 'Year': '2011', 'Wins': '37', 'Losses': '35', 'OT Losses': '10', 'Win %': '0.451', 'Goals For (GF)': '225', 'Goals Against (GA)': '246', '+ / -': '-21'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "Path('data/interim').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "with open('data/interim/hockey_teams.json', 'w') as f:\n",
        "    print(f\"Number of teams: {len(all_teams)}\")\n",
        "    print(\"First team:\", all_teams[0])\n",
        "    print(\"Last team:\", all_teams[-1])\n",
        "    json.dump(all_teams, f)  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
